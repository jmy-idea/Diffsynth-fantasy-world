# ğŸ¥ æ¨ç†ä¸åº”ç”¨å®Œæ•´æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨ Fantasy World æ¨¡å‹è¿›è¡Œæ¨ç†å’Œç”Ÿæˆè§†é¢‘ã€‚

---

## ğŸ“‹ ç›®å½•

1. [æ¨ç†åŸºç¡€](#æ¨ç†åŸºç¡€)
2. [æ¨¡å‹åŠ è½½](#æ¨¡å‹åŠ è½½)
3. [ç›¸æœºè½¨è¿¹æ§åˆ¶](#ç›¸æœºè½¨è¿¹æ§åˆ¶)
4. [è§†é¢‘ç”Ÿæˆ](#è§†é¢‘ç”Ÿæˆ)
5. [æ‰¹å¤„ç†ä¸ä¼˜åŒ–](#æ‰¹å¤„ç†ä¸ä¼˜åŒ–)
6. [è¾“å‡ºå¤„ç†](#è¾“å‡ºå¤„ç†)
7. [æ¨ç†ç¤ºä¾‹](#æ¨ç†ç¤ºä¾‹)

---

## ğŸš€ æ¨ç†åŸºç¡€

### ä»€ä¹ˆæ˜¯æ¨ç†ï¼Ÿ

æ¨ç†æ˜¯æŒ‡ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆæ–°æ•°æ®çš„è¿‡ç¨‹ï¼š

```
è¾“å…¥ (æ–‡æœ¬æç¤º + å›¾åƒ/ç›¸æœºè½¨è¿¹) 
    â†“
[å·²è®­ç»ƒçš„ Fantasy World æ¨¡å‹]
    â†“
è¾“å‡º (ç”Ÿæˆçš„è§†é¢‘ + å‡ ä½•é¢„æµ‹)
```

### æ¨ç† vs è®­ç»ƒ

| ç‰¹æ€§ | è®­ç»ƒ | æ¨ç† |
|------|------|------|
| **GPU éœ€æ±‚** | 8 Ã— H20 (40GB) | 1 Ã— ä»»ä½• GPU (12GB+) |
| **å†…å­˜è¦æ±‚** | é«˜ | ä¸­ç­‰ |
| **æ—¶é—´** | 200 å°æ—¶ | 1-2 åˆ†é’Ÿ / è§†é¢‘ |
| **å¯ç¼–è¾‘** | æ˜¯ | å¦ |
| **ç”¨é€”** | æ”¹è¿›æ¨¡å‹ | ç”Ÿæˆç»“æœ |

### æ¨ç†æ¨¡å¼

Fantasy World æ”¯æŒå¤šç§æ¨ç†æ¨¡å¼ï¼š

| æ¨¡å¼ | è¯´æ˜ | éœ€è¦çš„è¾“å…¥ | è¾“å‡º |
|------|------|---------|------|
| **æ–‡æœ¬åˆ°è§†é¢‘** | åŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆ | æ–‡æœ¬ + ç›¸æœºè½¨è¿¹ | è§†é¢‘ + æ·±åº¦ + ç‚¹äº‘ |
| **å›¾åƒåˆ°è§†é¢‘** | ä»èµ·å§‹å¸§æ‰©å±•è§†é¢‘ | å›¾åƒ + ç›¸æœºè½¨è¿¹ | è§†é¢‘ + å‡ ä½• |
| **ç›¸æœºæ§åˆ¶** | ç²¾ç¡®æ§åˆ¶æ‘„åƒæœºè·¯å¾„ | æ–‡æœ¬ + ç›¸æœºæ–‡ä»¶ | ç›¸æœºå—æ§è§†é¢‘ |

---

## ğŸ”Œ æ¨¡å‹åŠ è½½

### å‰ç½®è¦æ±‚

```bash
# 1. æ£€æŸ¥ç¯å¢ƒ
python -c "import diffsynth; print('OK')"

# 2. æ£€æŸ¥ GPU å¯ç”¨æ€§
python -c "import torch; print(torch.cuda.is_available())"

# 3. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls outputs/fantasy_world_stage2/step-10000.safetensors
```

### æ–¹æ³• 1: ä»æœ¬åœ°æ£€æŸ¥ç‚¹åŠ è½½ (æ¨è)

```python
import torch
from diffsynth import WanVideoPipeline

# 1. åˆå§‹åŒ–åŸºç¡€æ¨¡å‹
pipe = WanVideoPipeline.from_pretrained(
    "PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera",
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨ BFloat16 èŠ‚çœæ˜¾å­˜
    device_map="cuda:0"           # ä½¿ç”¨ GPU 0
)

# 2. å¯ç”¨ Fantasy World æ¨¡å¼
pipe.dit.enable_fantasy_world_mode(training_stage="stage2")

# 3. åŠ è½½å¾®è°ƒçš„æ£€æŸ¥ç‚¹
checkpoint_path = "outputs/fantasy_world_stage2/step-10000.safetensors"
state_dict = torch.load(checkpoint_path, map_location="cpu")

# è½¬æ¢ä¸º float32 (å¦‚æœæ¨¡å‹æ˜¯ float32)
state_dict = {k: v.to(torch.bfloat16) if v.dtype == torch.float32 else v 
              for k, v in state_dict.items()}

# åŠ è½½çŠ¶æ€å­—å…¸
# strict=False: å…è®¸ç¼ºå¤±çš„é”® (frozen Wan blocks)
pipe.dit.load_state_dict(state_dict, strict=False)

print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
```

### æ–¹æ³• 2: ä½¿ç”¨æ¨ç†è„šæœ¬ (æœ€ç®€å•)

```bash
# ä½¿ç”¨é¢„å‡†å¤‡çš„æ¨ç†è„šæœ¬
python examples/wanvideo/model_inference/fantasy_world_inference.py \
    --checkpoint outputs/fantasy_world_stage2/step-10000.safetensors \
    --prompt "a serene indoor scene with camera slowly rotating" \
    --output_dir results/ \
    --num_frames 81 \
    --seed 42
```

### æ–¹æ³• 3: é€šè¿‡å‘½ä»¤è¡ŒåŒ…è£…å™¨

```bash
# åˆ›å»ºä¾¿æ·è„šæœ¬
cat > run_inference.py << 'EOF'
#!/usr/bin/env python
import argparse
from diffsynth_fantasy_world import run_inference

parser = argparse.ArgumentParser()
parser.add_argument("--prompt", required=True, help="ç”Ÿæˆæç¤º")
parser.add_argument("--checkpoint", default="outputs/fantasy_world_stage2/step-10000.safetensors")
parser.add_argument("--output_dir", default="results/")
parser.add_argument("--num_frames", type=int, default=81)
parser.add_argument("--seed", type=int, default=42)

args = parser.parse_args()
run_inference(**vars(args))
EOF

python run_inference.py --prompt "a camera moving through a room"
```

---

## ğŸ“· ç›¸æœºè½¨è¿¹æ§åˆ¶

### ç›¸æœºè½¨è¿¹æ–‡ä»¶æ ¼å¼

ç›¸æœºè½¨è¿¹æ˜¯ä¸€ä¸ª `.txt` æ–‡ä»¶ï¼Œæ¯è¡Œ 19 ä¸ªå€¼ï¼š

```
frame_idx fx fy cx cy k1 k2 w2c_00 w2c_01 ... w2c_23

å…¶ä¸­:
- frame_idx: å¸§åºå· (0-T)
- fx, fy: ç„¦è· (å†…å‚)
- cx, cy: ä¸»ç‚¹ (å†…å‚)
- k1, k2: å¾„å‘ç•¸å˜ç³»æ•°
- w2c_*: ä¸–ç•Œåˆ°ç›¸æœºçš„ 3Ã—4 çŸ©é˜µ (12 ä¸ªå€¼)
```

**ç¤ºä¾‹**:
```
0 500.0 500.0 320.0 240.0 0.0 0.0 0.9 0.1 0.2 1.0 0.3 -0.5 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2
1 500.0 500.0 320.0 240.0 0.0 0.0 0.85 0.12 0.25 1.1 0.32 -0.48 0.42 0.52 0.62 0.72 0.82 0.92 1.02 1.12 1.22
...
```

### ç”Ÿæˆé¢„å®šä¹‰è½¨è¿¹

Fantasy World æä¾›å¤šç§é¢„å®šä¹‰è½¨è¿¹ï¼š

#### 1. è½¨é“è¿åŠ¨ (Orbital Motion)

```python
import numpy as np

def create_orbital_trajectory(num_frames, radius=3.0, height=1.5):
    """
    åˆ›å»ºç»•åœºæ™¯æ—‹è½¬çš„è½¨è¿¹
    
    Args:
        num_frames: æ€»å¸§æ•°
        radius: æ—‹è½¬åŠå¾„ (ç±³)
        height: ç›¸æœºé«˜åº¦ (ç±³)
    """
    trajectories = []
    
    for i in range(num_frames):
        t = i / num_frames * 2 * np.pi  # 0 åˆ° 2Ï€
        
        # ä½ç½® (ç»• Y è½´æ—‹è½¬)
        x = radius * np.cos(t)
        y = height
        z = radius * np.sin(t)
        
        # æ³¨è§†ç‚¹ (åœºæ™¯ä¸­å¿ƒ)
        look_at = np.array([0, height, 0])
        
        # æ„å»º w2c çŸ©é˜µ (é€šå¸¸ç”± COLMAP æˆ–æ‰‹åŠ¨æŒ‡å®š)
        # è¿™é‡Œç®€åŒ–å¤„ç†...
        
        trajectories.append({
            'frame': i,
            'position': np.array([x, y, z]),
            'look_at': look_at
        })
    
    return trajectories
```

#### 2. å‰å‘è¿åŠ¨ (Forward Motion)

```python
def create_forward_trajectory(num_frames, start_z=10.0, end_z=0.5):
    """ç›¸æœºå‘å‰ç§»åŠ¨"""
    trajectories = []
    
    for i in range(num_frames):
        t = i / (num_frames - 1)
        z = start_z * (1 - t) + end_z * t  # çº¿æ€§æ’å€¼
        
        trajectories.append({
            'frame': i,
            'position': np.array([0, 0, z]),
            'look_at': np.array([0, 0, 0])
        })
    
    return trajectories
```

#### 3. è‡ªå®šä¹‰è½¨è¿¹

```python
def create_custom_trajectory(num_frames, keyframes):
    """
    ä»å…³é”®å¸§æ’å€¼ç”Ÿæˆè½¨è¿¹
    
    Args:
        keyframes: {frame_idx: (x, y, z), ...}
    """
    trajectories = []
    frame_indices = sorted(keyframes.keys())
    
    for i in range(num_frames):
        # æ‰¾åˆ°ç›¸é‚»çš„å…³é”®å¸§
        if i <= frame_indices[0]:
            pos = keyframes[frame_indices[0]]
        elif i >= frame_indices[-1]:
            pos = keyframes[frame_indices[-1]]
        else:
            # çº¿æ€§æ’å€¼
            for j in range(len(frame_indices) - 1):
                f1, f2 = frame_indices[j], frame_indices[j + 1]
                if f1 <= i <= f2:
                    p1 = np.array(keyframes[f1])
                    p2 = np.array(keyframes[f2])
                    alpha = (i - f1) / (f2 - f1)
                    pos = p1 * (1 - alpha) + p2 * alpha
                    break
        
        trajectories.append({
            'frame': i,
            'position': pos,
            'look_at': np.array([0, 0, 0])
        })
    
    return trajectories
```

### ä¿å­˜è½¨è¿¹æ–‡ä»¶

```python
def save_trajectory_file(trajectories, output_file, intrinsics=None):
    """
    ä¿å­˜è½¨è¿¹ä¸º .txt æ–‡ä»¶
    
    Args:
        trajectories: è½¨è¿¹åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        intrinsics: ç›¸æœºå†…å‚ {'fx': ..., 'fy': ..., ...}
    """
    if intrinsics is None:
        intrinsics = {
            'fx': 500.0,
            'fy': 500.0,
            'cx': 320.0,
            'cy': 240.0,
            'k1': 0.0,
            'k2': 0.0
        }
    
    with open(output_file, 'w') as f:
        for traj in trajectories:
            frame_idx = traj['frame']
            
            # æ„å»º w2c çŸ©é˜µ (éœ€è¦ä»ä½ç½®å’Œæ³¨è§†ç‚¹è®¡ç®—)
            # è¿™é‡Œç®€åŒ–ä¸ºç¤ºä¾‹çŸ©é˜µ
            w2c = np.eye(3, 4)  # [3, 4]
            
            # ç»„è£… 19 å€¼
            line = f"{frame_idx} "
            line += f"{intrinsics['fx']} {intrinsics['fy']} "
            line += f"{intrinsics['cx']} {intrinsics['cy']} "
            line += f"{intrinsics['k1']} {intrinsics['k2']} "
            line += " ".join(map(str, w2c.flatten()))
            
            f.write(line + "\n")
    
    print(f"âœ… è½¨è¿¹å·²ä¿å­˜: {output_file}")

# ä½¿ç”¨ç¤ºä¾‹
orbital_traj = create_orbital_trajectory(num_frames=81)
save_trajectory_file(orbital_traj, "trajectory_orbital.txt")
```

---

## ğŸ¬ è§†é¢‘ç”Ÿæˆ

### åŸºç¡€ç”Ÿæˆ

```python
import torch
from diffsynth import WanVideoPipeline

# åŠ è½½æ¨¡å‹ (è§å‰é¢çš„æ¨¡å‹åŠ è½½éƒ¨åˆ†)
pipe = WanVideoPipeline.from_pretrained(...)
pipe.dit.enable_fantasy_world_mode(training_stage="stage2")
# ... åŠ è½½æ£€æŸ¥ç‚¹

# ç”Ÿæˆè§†é¢‘
video = pipe(
    prompt="a beautiful living room with sunlight coming through windows",
    negative_prompt="blurry, low quality",
    num_frames=81,
    height=336,
    width=592,
    num_inference_steps=50,
    guidance_scale=7.5,
    seed=42,
)

print(f"ç”Ÿæˆè§†é¢‘å½¢çŠ¶: {video.shape}")  # [81, 3, 336, 592]
```

### å¸¦ç›¸æœºæ§åˆ¶çš„ç”Ÿæˆ

```python
# ä½¿ç”¨ç›¸æœºè½¨è¿¹æ§åˆ¶ç”Ÿæˆ
video = pipe(
    prompt="a camera moving through an abandoned building",
    pose_file_path="trajectory_orbital.txt",  # ç›¸æœºè½¨è¿¹
    num_frames=81,
    height=336,
    width=592,
    num_inference_steps=50,
    guidance_scale=7.5,
    seed=42,
)
```

### å‚æ•°è¯¦è§£

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `prompt` | str | - | **å¿…éœ€**: æ–‡æœ¬æè¿° |
| `negative_prompt` | str | "" | è¦é¿å…çš„æè¿° |
| `num_frames` | int | 81 | ç”Ÿæˆå¸§æ•° (21-81) |
| `height` | int | 336 | è§†é¢‘é«˜åº¦ (å¤š 16 å€æ•°) |
| `width` | int | 592 | è§†é¢‘å®½åº¦ (å¤š 16 å€æ•°) |
| `num_inference_steps` | int | 50 | æ¨ç†æ­¥æ•° (è¶Šå¤šè¶Šå¥½ï¼Œè¶Šæ…¢) |
| `guidance_scale` | float | 7.5 | å¼•å¯¼å¼ºåº¦ (1-20) |
| `seed` | int | - | éšæœºç§å­ (å¯å¤ç°) |
| `pose_file_path` | str | None | ç›¸æœºè½¨è¿¹æ–‡ä»¶ |
| `generator` | Generator | None | PyTorch éšæœºç”Ÿæˆå™¨ |

### æ¨ç†é€Ÿåº¦ vs è´¨é‡

```python
# å¿«é€Ÿä½†è´¨é‡ä¸€èˆ¬ (30 æ­¥, 1-2 åˆ†é’Ÿ)
video_fast = pipe(
    prompt="...",
    num_inference_steps=30,
    guidance_scale=5.0
)

# å¹³è¡¡ (50 æ­¥, 2-3 åˆ†é’Ÿ)
video_balanced = pipe(
    prompt="...",
    num_inference_steps=50,
    guidance_scale=7.5
)

# è´¨é‡ä¼˜å…ˆ (70 æ­¥, 4-5 åˆ†é’Ÿ)
video_quality = pipe(
    prompt="...",
    num_inference_steps=70,
    guidance_scale=9.0
)
```

---

## ğŸš„ æ‰¹å¤„ç†ä¸ä¼˜åŒ–

### æ‰¹é‡ç”Ÿæˆ

```python
import torch
from diffsynth import WanVideoPipeline

pipe = WanVideoPipeline.from_pretrained(...)
pipe.dit.enable_fantasy_world_mode(training_stage="stage2")
# ... åŠ è½½æ£€æŸ¥ç‚¹

# æç¤ºåˆ—è¡¨
prompts = [
    "a minimalist interior with soft lighting",
    "a vibrant marketplace with people and activity",
    "a serene nature scene with flowing water",
]

# ç”Ÿæˆæ‰€æœ‰è§†é¢‘
for i, prompt in enumerate(prompts):
    print(f"ç”Ÿæˆè§†é¢‘ {i+1}/{len(prompts)}...")
    
    video = pipe(
        prompt=prompt,
        num_frames=81,
        seed=42 + i  # ä¸åŒç§å­ä¿è¯å¤šæ ·æ€§
    )
    
    # ä¿å­˜ (è§åé¢çš„è¾“å‡ºå¤„ç†)
    save_video(video, f"results/video_{i:02d}.mp4")
```

### æ˜¾å­˜ä¼˜åŒ–

**æ–¹æ³• 1: å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›**

```python
# åŠ è½½æ¨¡å‹æ—¶å¯ç”¨
pipe = WanVideoPipeline.from_pretrained(
    ...,
    enable_attention_slicing=True  # èŠ‚çœæ˜¾å­˜
)
```

**æ–¹æ³• 2: å‡å°‘æ¨ç†æ­¥æ•°**

```python
# ä» 50 æ­¥é™åˆ° 30 æ­¥ (æ˜¾å­˜å‡å°‘ 40%, é€Ÿåº¦å¿« 40%)
video = pipe(..., num_inference_steps=30)
```

**æ–¹æ³• 3: é™ä½åˆ†è¾¨ç‡**

```python
# ä» 336Ã—592 é™åˆ° 224Ã—384 (æ˜¾å­˜å‡å°‘ ~50%)
video = pipe(
    ...,
    height=224,
    width=384,
    num_frames=41  # ä¹Ÿå¯å‡å°‘å¸§æ•°
)
```

**æ–¹æ³• 4: ä½¿ç”¨ CPU offloading**

```python
# åœ¨ GPU å’Œ CPU ä¹‹é—´è½¬ç§»æ¨¡å—
pipe.enable_model_cpu_offload()  # ç¨å¾®æ…¢ä¸€äº›ï¼Œä½†æ˜¾å­˜å‡å°‘
```

### æ¨ç†åŠ é€Ÿ

**æ–¹æ³• 1: ä½¿ç”¨ BFloat16**

```python
pipe = WanVideoPipeline.from_pretrained(
    ...,
    torch_dtype=torch.bfloat16  # åŠ é€Ÿ + èŠ‚çœæ˜¾å­˜
)
```

**æ–¹æ³• 2: å¯ç”¨ xFormers ä¼˜åŒ–**

```bash
pip install xformers

python << 'EOF'
from diffsynth import WanVideoPipeline
pipe = WanVideoPipeline.from_pretrained(...)
pipe.enable_xformers_memory_efficient_attention()
EOF
```

---

## ğŸ’¾ è¾“å‡ºå¤„ç†

### ä¿å­˜è§†é¢‘

```python
import cv2
import numpy as np
from pathlib import Path

def save_video(video_tensor, output_path, fps=24):
    """
    ä¿å­˜ç”Ÿæˆçš„è§†é¢‘
    
    Args:
        video_tensor: [T, 3, H, W] æˆ– [T, H, W, 3] å¼ é‡
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (.mp4)
        fps: å¸§ç‡
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # è½¬æ¢å¼ é‡æ ¼å¼
    if isinstance(video_tensor, torch.Tensor):
        video_tensor = video_tensor.cpu().numpy()
    
    # ç¡®ä¿å½¢çŠ¶ä¸º [T, H, W, 3]
    if video_tensor.shape[1] == 3:
        video_tensor = video_tensor.transpose(0, 2, 3, 1)
    
    # è½¬æ¢å€¼åŸŸ [0, 1] â†’ [0, 255]
    if video_tensor.max() <= 1.0:
        video_tensor = (video_tensor * 255).astype(np.uint8)
    else:
        video_tensor = video_tensor.astype(np.uint8)
    
    # åˆå§‹åŒ– VideoWriter
    height, width = video_tensor.shape[1:3]
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(
        str(output_path),
        fourcc,
        fps,
        (width, height)
    )
    
    # é€å¸§å†™å…¥
    for frame in video_tensor:
        # è½¬æ¢ RGB â†’ BGR (OpenCV æ ¼å¼)
        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        writer.write(frame_bgr)
    
    writer.release()
    print(f"âœ… è§†é¢‘å·²ä¿å­˜: {output_path}")

# ä½¿ç”¨
video = pipe(prompt="...")  # [T, 3, H, W]
save_video(video, "results/output.mp4", fps=24)
```

### ä¿å­˜ 3D å‡ ä½•

```python
def save_depth_map(depth_tensor, output_dir):
    """ä¿å­˜é¢„æµ‹çš„æ·±åº¦å›¾"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # depth_tensor: [T, 1, H, W] æˆ– [T, H, W]
    if depth_tensor.dim() == 4:
        depth_tensor = depth_tensor.squeeze(1)
    
    for i, depth in enumerate(depth_tensor):
        # è½¬æ¢ä¸º numpy å¹¶å½’ä¸€åŒ–
        depth_np = depth.cpu().numpy()
        depth_np = (depth_np * 255).astype(np.uint8)
        
        # ä¿å­˜ä¸º PNG
        output_path = output_dir / f"depth_{i:04d}.png"
        cv2.imwrite(str(output_path), depth_np)
    
    print(f"âœ… æ·±åº¦å›¾å·²ä¿å­˜: {output_dir}")

def save_point_cloud(points_tensor, output_dir):
    """ä¿å­˜é¢„æµ‹çš„ç‚¹äº‘"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # points_tensor: [T, 3, H, W]
    for i, points in enumerate(points_tensor):
        # è½¬æ¢ä¸º [H*W, 3] æ ¼å¼
        points_np = points.permute(1, 2, 0).cpu().numpy()  # [H, W, 3]
        points_flat = points_np.reshape(-1, 3)
        
        # ä¿å­˜ä¸º PLY (ç‚¹äº‘æ ¼å¼)
        output_path = output_dir / f"points_{i:04d}.ply"
        save_ply(points_flat, str(output_path))
    
    print(f"âœ… ç‚¹äº‘å·²ä¿å­˜: {output_dir}")
```

### ç”Ÿæˆå¯è§†åŒ–

```python
def visualize_output(video, depth, points, output_path):
    """ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–"""
    import matplotlib.pyplot as plt
    
    # é€‰æ‹©å…³é”®å¸§
    key_frames = [0, len(video) // 2, -1]
    
    fig, axes = plt.subplots(3, len(key_frames), figsize=(15, 10))
    
    for col, frame_idx in enumerate(key_frames):
        # è§†é¢‘å¸§
        frame = video[frame_idx].permute(1, 2, 0).cpu().numpy()
        axes[0, col].imshow(frame)
        axes[0, col].set_title(f"Frame {frame_idx}")
        
        # æ·±åº¦å›¾
        depth_map = depth[frame_idx, 0].cpu().numpy()
        axes[1, col].imshow(depth_map, cmap='viridis')
        axes[1, col].set_title("Depth")
        
        # ç‚¹äº‘ (å¯è§†åŒ–ä¸º 3D)
        points_map = points[frame_idx].cpu().numpy()
        # è¿™é‡Œç®€åŒ–ä¸ºæ˜¾ç¤º X åˆ†é‡
        axes[2, col].imshow(points_map[0], cmap='coolwarm')
        axes[2, col].set_title("Points (X)")
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150)
    print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
```

---

## ğŸ“ æ¨ç†ç¤ºä¾‹

### ç¤ºä¾‹ 1: ç®€å•æ–‡æœ¬åˆ°è§†é¢‘

```python
#!/usr/bin/env python
import torch
from diffsynth import WanVideoPipeline

# åŠ è½½æ¨¡å‹
pipe = WanVideoPipeline.from_pretrained(
    "PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera",
    torch_dtype=torch.bfloat16,
    device_map="cuda:0"
)

# å¯ç”¨ Fantasy World
pipe.dit.enable_fantasy_world_mode(training_stage="stage2")

# åŠ è½½æ£€æŸ¥ç‚¹
state_dict = torch.load("outputs/fantasy_world_stage2/step-10000.safetensors", map_location="cpu")
pipe.dit.load_state_dict(state_dict, strict=False)

# ç”Ÿæˆè§†é¢‘
video = pipe(
    prompt="a beautiful garden with flowers and butterflies",
    num_frames=81,
    num_inference_steps=50,
    guidance_scale=7.5,
    seed=42
)

# ä¿å­˜
save_video(video, "results/garden.mp4", fps=24)
```

### ç¤ºä¾‹ 2: å¸¦ç›¸æœºæ§åˆ¶çš„ç”Ÿæˆ

```python
#!/usr/bin/env python
import torch
from diffsynth import WanVideoPipeline
import numpy as np

# åŠ è½½æ¨¡å‹ (åŒä¸Š)
pipe = WanVideoPipeline.from_pretrained(...)
# ... åˆå§‹åŒ–ä»£ç 

# åˆ›å»ºç›¸æœºè½¨è¿¹
def create_camera_trajectory(num_frames):
    cameras = []
    for i in range(num_frames):
        # ç®€åŒ–çš„ç›¸æœºå‚æ•° (å®é™…éœ€è¦å®Œæ•´çš„ w2c çŸ©é˜µ)
        line = f"{i} 500.0 500.0 320.0 240.0 0.0 0.0 "
        line += " ".join([str(float(j)) for j in range(12)])
        cameras.append(line)
    
    with open("trajectory.txt", "w") as f:
        f.write("\n".join(cameras))

create_camera_trajectory(81)

# ç”Ÿæˆè§†é¢‘
video = pipe(
    prompt="a camera slowly panning through an abandoned castle",
    pose_file_path="trajectory.txt",
    num_frames=81,
    num_inference_steps=50,
    seed=42
)

save_video(video, "results/castle_pan.mp4")
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### æç¤ºè¯è®¾è®¡

**å¥½çš„æç¤ºè¯**:
- âœ… "a serene living room with soft warm lighting"
- âœ… "a camera orbiting a modern sculpture"
- âœ… "people walking through a busy marketplace"

**ä¸å¥½çš„æç¤ºè¯**:
- âŒ "room" (å¤ªç®€æ´)
- âŒ "something moving in a place" (å¤ªæ¨¡ç³Š)
- âŒ è¶…è¿‡ 100 å­— (å¤ªé•¿)

### ä¼˜åŒ–ç­–ç•¥

1. **å“è´¨ä¼˜åŒ–**: å¢åŠ  inference steps (50-70)
2. **é€Ÿåº¦ä¼˜åŒ–**: å‡å°‘ frames (41 è€Œé 81) æˆ– steps (30)
3. **ç¨³å®šæ€§**: è®¾ç½®ç§å­ç¡®ä¿å¯å¤ç°
4. **å¤šæ ·æ€§**: æ”¹å˜ç§å­ç”Ÿæˆå¤šä¸ªå˜ä½“

### æ•…éšœæ’æŸ¥

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| "æ˜¾å­˜ä¸è¶³" | æ¨¡å‹å¤ªå¤§ | å¯ç”¨ CPU offloading æˆ–é™ä½åˆ†è¾¨ç‡ |
| "ç”Ÿæˆé‡å¤å¸§" | æ¨¡å‹æ¬ æ‹Ÿåˆ | å¢åŠ  inference steps |
| "è§†é¢‘æŠ–åŠ¨" | ä¸ç¨³å®š | å¢åŠ  guidance scale æˆ–æ”¹å˜ seed |
| "å‡ ä½•æ— æ•ˆ" | æ£€æŸ¥ç‚¹é—®é¢˜ | é‡æ–°åŠ è½½æˆ–éªŒè¯æ£€æŸ¥ç‚¹ |

---

**ä¸‹ä¸€æ­¥**: å¦‚é‡é—®é¢˜ï¼ŒæŸ¥çœ‹ [æ•…éšœæ’æŸ¥](./TROUBLESHOOTING.md) æˆ– [æŠ€æœ¯æ·±å…¥](./TECHNICAL_DEEP_DIVE.md)ã€‚
